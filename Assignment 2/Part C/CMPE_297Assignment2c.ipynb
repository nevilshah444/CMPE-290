{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Do an link  prediction of graph Assignment\n"
      ],
      "metadata": {
        "id": "ws2q4_aDh1XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "5N29LNidx5YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f68eb7b-03b4-413c-c839-8f0aaee49bbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=912c184fd9a1a5e906b1be05aea9a776c3975e6784a8810c67fbc24abbe879ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_sparse"
      ],
      "metadata": {
        "id": "FNmwi5ldyEso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0472e2-ff55-40d8-b2b6-86fd4a624f08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_sparse\n",
            "  Downloading torch_sparse-0.6.15.tar.gz (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch_sparse) (1.21.6)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl size=1577340 sha256=6786cf6eb71f4921cb1b9f6499d4a5340028ad364f232aca5f48d2b0280735c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/68/4d/1414be5c2c622bad35364e13213180797717b6d4b8923936dc\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_scatter"
      ],
      "metadata": {
        "id": "EdgaFd3H2tb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b74370-c3f3-4e7c-ecb6-3bc5793c01bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=2657113 sha256=bbf792882eb095f53261781f98150e1f8c1ac55cb15165370a4f1a5fe2aa5ba6\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import negative_sampling"
      ],
      "metadata": {
        "id": "cgS17-QU4XZ9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DFjVOmUB4dW3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToDevice(device),\n",
        "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
        "                      add_negative_train_samples=False),\n",
        "])"
      ],
      "metadata": {
        "id": "TKXw1D9o4e76"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'Cora'\n",
        "path = osp.join('.', 'data', dataset)\n",
        "#path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
        "dataset = Planetoid(path, name='Cora', transform=transform)"
      ],
      "metadata": {
        "id": "a4LU8OBy4hLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358aa3a1-0814-4c61-fc97-12591de849b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data, val_data, test_data = dataset[0]"
      ],
      "metadata": {
        "id": "KnCYSZ1Y49xH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_label_index):\n",
        "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z):\n",
        "        prob_adj = z @ z.t()\n",
        "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
      ],
      "metadata": {
        "id": "YIxW4OgB5A0F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(dataset.num_features, 128, 64).to(device)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "4MzN6D335B0S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(train_data.x, train_data.edge_index)\n",
        "\n",
        "    # We perform a new round of negative sampling for every training epoch:\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
        "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
        "\n",
        "    edge_label_index = torch.cat(\n",
        "        [train_data.edge_label_index, neg_edge_index],\n",
        "        dim=-1,\n",
        "    )\n",
        "    edge_label = torch.cat([\n",
        "        train_data.edge_label,\n",
        "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
        "    ], dim=0)\n",
        "\n",
        "    out = model.decode(z, edge_label_index).view(-1)\n",
        "    loss = criterion(out, edge_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "wVHtaLXC5FTt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
        "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
        "\n",
        "\n",
        "best_val_auc = final_test_auc = 0\n",
        "for epoch in range(1, 101):\n",
        "    loss = train()\n",
        "    val_auc = test(val_data)\n",
        "    test_auc = test(test_data)\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        final_test_auc = test_auc\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
        "          f'Test: {test_auc:.4f}')"
      ],
      "metadata": {
        "id": "MiEniC1J5F8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2112fe64-6800-4017-bb4a-0172cc7a8d25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.6930, Val: 0.7178, Test: 0.6720\n",
            "Epoch: 002, Loss: 0.6805, Val: 0.7082, Test: 0.6621\n",
            "Epoch: 003, Loss: 0.7216, Val: 0.7014, Test: 0.6653\n",
            "Epoch: 004, Loss: 0.6756, Val: 0.6917, Test: 0.6762\n",
            "Epoch: 005, Loss: 0.6849, Val: 0.6946, Test: 0.6922\n",
            "Epoch: 006, Loss: 0.6895, Val: 0.6947, Test: 0.6983\n",
            "Epoch: 007, Loss: 0.6910, Val: 0.6895, Test: 0.6945\n",
            "Epoch: 008, Loss: 0.6912, Val: 0.6845, Test: 0.6856\n",
            "Epoch: 009, Loss: 0.6907, Val: 0.6801, Test: 0.6795\n",
            "Epoch: 010, Loss: 0.6894, Val: 0.6763, Test: 0.6749\n",
            "Epoch: 011, Loss: 0.6871, Val: 0.6704, Test: 0.6688\n",
            "Epoch: 012, Loss: 0.6833, Val: 0.6658, Test: 0.6631\n",
            "Epoch: 013, Loss: 0.6781, Val: 0.6630, Test: 0.6584\n",
            "Epoch: 014, Loss: 0.6751, Val: 0.6576, Test: 0.6543\n",
            "Epoch: 015, Loss: 0.6758, Val: 0.6547, Test: 0.6530\n",
            "Epoch: 016, Loss: 0.6715, Val: 0.6555, Test: 0.6579\n",
            "Epoch: 017, Loss: 0.6637, Val: 0.6677, Test: 0.6741\n",
            "Epoch: 018, Loss: 0.6586, Val: 0.6823, Test: 0.6965\n",
            "Epoch: 019, Loss: 0.6553, Val: 0.6910, Test: 0.7070\n",
            "Epoch: 020, Loss: 0.6475, Val: 0.6911, Test: 0.7040\n",
            "Epoch: 021, Loss: 0.6373, Val: 0.6909, Test: 0.7023\n",
            "Epoch: 022, Loss: 0.6287, Val: 0.7052, Test: 0.7147\n",
            "Epoch: 023, Loss: 0.6189, Val: 0.7427, Test: 0.7346\n",
            "Epoch: 024, Loss: 0.6076, Val: 0.7743, Test: 0.7455\n",
            "Epoch: 025, Loss: 0.5974, Val: 0.7819, Test: 0.7444\n",
            "Epoch: 026, Loss: 0.5908, Val: 0.7779, Test: 0.7423\n",
            "Epoch: 027, Loss: 0.5829, Val: 0.7725, Test: 0.7404\n",
            "Epoch: 028, Loss: 0.5768, Val: 0.7768, Test: 0.7393\n",
            "Epoch: 029, Loss: 0.5737, Val: 0.7751, Test: 0.7360\n",
            "Epoch: 030, Loss: 0.5702, Val: 0.7762, Test: 0.7387\n",
            "Epoch: 031, Loss: 0.5726, Val: 0.7759, Test: 0.7412\n",
            "Epoch: 032, Loss: 0.5823, Val: 0.7761, Test: 0.7415\n",
            "Epoch: 033, Loss: 0.5575, Val: 0.7754, Test: 0.7412\n",
            "Epoch: 034, Loss: 0.5675, Val: 0.7754, Test: 0.7442\n",
            "Epoch: 035, Loss: 0.5568, Val: 0.7757, Test: 0.7481\n",
            "Epoch: 036, Loss: 0.5648, Val: 0.7779, Test: 0.7505\n",
            "Epoch: 037, Loss: 0.5561, Val: 0.7804, Test: 0.7534\n",
            "Epoch: 038, Loss: 0.5490, Val: 0.7839, Test: 0.7569\n",
            "Epoch: 039, Loss: 0.5525, Val: 0.7869, Test: 0.7605\n",
            "Epoch: 040, Loss: 0.5471, Val: 0.7904, Test: 0.7640\n",
            "Epoch: 041, Loss: 0.5446, Val: 0.7930, Test: 0.7669\n",
            "Epoch: 042, Loss: 0.5395, Val: 0.7948, Test: 0.7688\n",
            "Epoch: 043, Loss: 0.5366, Val: 0.7976, Test: 0.7699\n",
            "Epoch: 044, Loss: 0.5376, Val: 0.8013, Test: 0.7741\n",
            "Epoch: 045, Loss: 0.5400, Val: 0.8038, Test: 0.7792\n",
            "Epoch: 046, Loss: 0.5292, Val: 0.8094, Test: 0.7841\n",
            "Epoch: 047, Loss: 0.5273, Val: 0.8162, Test: 0.7902\n",
            "Epoch: 048, Loss: 0.5207, Val: 0.8226, Test: 0.7966\n",
            "Epoch: 049, Loss: 0.5262, Val: 0.8289, Test: 0.8041\n",
            "Epoch: 050, Loss: 0.5198, Val: 0.8320, Test: 0.8102\n",
            "Epoch: 051, Loss: 0.5126, Val: 0.8347, Test: 0.8152\n",
            "Epoch: 052, Loss: 0.5054, Val: 0.8369, Test: 0.8177\n",
            "Epoch: 053, Loss: 0.5158, Val: 0.8404, Test: 0.8178\n",
            "Epoch: 054, Loss: 0.5124, Val: 0.8422, Test: 0.8192\n",
            "Epoch: 055, Loss: 0.5105, Val: 0.8446, Test: 0.8227\n",
            "Epoch: 056, Loss: 0.5025, Val: 0.8457, Test: 0.8251\n",
            "Epoch: 057, Loss: 0.5072, Val: 0.8445, Test: 0.8255\n",
            "Epoch: 058, Loss: 0.5023, Val: 0.8433, Test: 0.8234\n",
            "Epoch: 059, Loss: 0.5007, Val: 0.8416, Test: 0.8209\n",
            "Epoch: 060, Loss: 0.4993, Val: 0.8401, Test: 0.8211\n",
            "Epoch: 061, Loss: 0.5021, Val: 0.8393, Test: 0.8231\n",
            "Epoch: 062, Loss: 0.5061, Val: 0.8382, Test: 0.8242\n",
            "Epoch: 063, Loss: 0.4947, Val: 0.8373, Test: 0.8251\n",
            "Epoch: 064, Loss: 0.4926, Val: 0.8381, Test: 0.8256\n",
            "Epoch: 065, Loss: 0.4960, Val: 0.8407, Test: 0.8256\n",
            "Epoch: 066, Loss: 0.4896, Val: 0.8428, Test: 0.8263\n",
            "Epoch: 067, Loss: 0.4978, Val: 0.8447, Test: 0.8294\n",
            "Epoch: 068, Loss: 0.4870, Val: 0.8467, Test: 0.8333\n",
            "Epoch: 069, Loss: 0.4811, Val: 0.8472, Test: 0.8363\n",
            "Epoch: 070, Loss: 0.4889, Val: 0.8514, Test: 0.8387\n",
            "Epoch: 071, Loss: 0.4952, Val: 0.8564, Test: 0.8400\n",
            "Epoch: 072, Loss: 0.4870, Val: 0.8593, Test: 0.8432\n",
            "Epoch: 073, Loss: 0.4806, Val: 0.8601, Test: 0.8468\n",
            "Epoch: 074, Loss: 0.4912, Val: 0.8631, Test: 0.8510\n",
            "Epoch: 075, Loss: 0.4765, Val: 0.8670, Test: 0.8553\n",
            "Epoch: 076, Loss: 0.4782, Val: 0.8700, Test: 0.8592\n",
            "Epoch: 077, Loss: 0.4779, Val: 0.8721, Test: 0.8625\n",
            "Epoch: 078, Loss: 0.4700, Val: 0.8737, Test: 0.8658\n",
            "Epoch: 079, Loss: 0.4761, Val: 0.8761, Test: 0.8706\n",
            "Epoch: 080, Loss: 0.4691, Val: 0.8778, Test: 0.8746\n",
            "Epoch: 081, Loss: 0.4677, Val: 0.8784, Test: 0.8779\n",
            "Epoch: 082, Loss: 0.4662, Val: 0.8775, Test: 0.8794\n",
            "Epoch: 083, Loss: 0.4712, Val: 0.8772, Test: 0.8803\n",
            "Epoch: 084, Loss: 0.4706, Val: 0.8779, Test: 0.8819\n",
            "Epoch: 085, Loss: 0.4675, Val: 0.8791, Test: 0.8833\n",
            "Epoch: 086, Loss: 0.4724, Val: 0.8791, Test: 0.8840\n",
            "Epoch: 087, Loss: 0.4671, Val: 0.8790, Test: 0.8835\n",
            "Epoch: 088, Loss: 0.4650, Val: 0.8797, Test: 0.8833\n",
            "Epoch: 089, Loss: 0.4728, Val: 0.8806, Test: 0.8832\n",
            "Epoch: 090, Loss: 0.4609, Val: 0.8812, Test: 0.8835\n",
            "Epoch: 091, Loss: 0.4681, Val: 0.8833, Test: 0.8838\n",
            "Epoch: 092, Loss: 0.4622, Val: 0.8836, Test: 0.8827\n",
            "Epoch: 093, Loss: 0.4631, Val: 0.8841, Test: 0.8823\n",
            "Epoch: 094, Loss: 0.4645, Val: 0.8837, Test: 0.8814\n",
            "Epoch: 095, Loss: 0.4611, Val: 0.8828, Test: 0.8810\n",
            "Epoch: 096, Loss: 0.4637, Val: 0.8850, Test: 0.8816\n",
            "Epoch: 097, Loss: 0.4643, Val: 0.8854, Test: 0.8824\n",
            "Epoch: 098, Loss: 0.4626, Val: 0.8859, Test: 0.8828\n",
            "Epoch: 099, Loss: 0.4580, Val: 0.8858, Test: 0.8843\n",
            "Epoch: 100, Loss: 0.4593, Val: 0.8850, Test: 0.8854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BsCOxYhAxydz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f9f404-a3a8-4d5e-cccb-b497f6fa6217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test: 0.8828\n"
          ]
        }
      ],
      "source": [
        "print(f'Final Test: {final_test_auc:.4f}')\n",
        "\n",
        "z = model.encode(test_data.x, test_data.edge_index)\n",
        "final_edge_index = model.decode_all(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "\n",
        "\n",
        "*   http://cs230.stanford.edu/projects_spring_2020/reports/38854344.pdf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   https://colab.research.google.com/drive/1f2KVSVsk5f4dpZlxNE0RBIHowqDs3KSp#scrollTo=dQ98qUOAt5JK\n",
        "\n",
        "\n",
        "\n",
        "*  https://colab.sandbox.google.com/github/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial12/Tutorial12%20GAE%20for%20link%20prediction.ipynb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4bv4rPF-H3B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHwyGKnU-NAy"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}